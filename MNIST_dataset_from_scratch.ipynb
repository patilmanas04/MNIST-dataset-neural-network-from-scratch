{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8ee7699-a8c8-4069-8f9f-c480d6bdbddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f464c71-ae33-441e-a48b-f2b7b3ce4e1c",
   "metadata": {},
   "source": [
    "# Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de5afecc-a92b-4571-952d-b77a6262a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0., bias_regularizer_l1=0., weight_regularizer_l2=0., bias_regularizer_l2=0.):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1 # lambda\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1 # lambda\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2 # lambda\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2 # lambda\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(self.inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dL_dz):\n",
    "        self.dL_dw = np.dot(self.inputs.T, dL_dz)\n",
    "        self.dL_db = np.sum(dL_dz, axis=0, keepdims=True)\n",
    "        self.dL_dX = np.dot(dL_dz, self.weights.T)\n",
    "\n",
    "        if self.weight_regularizer_l1>0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dL_dw += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l1>0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dL_db += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        if self.weight_regularizer_l2>0:\n",
    "            dL2 = 2 * self.weight_regularizer_l2 * self.weights\n",
    "            self.dL_dw += dL2\n",
    "\n",
    "        if self.bias_regularizer_l2>0:\n",
    "            dL2 = 2 * self.bias_regularizer_l2 * self.biases\n",
    "            self.dL_db += dL2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062ebd6-fa27-4918-be2d-9193c4cf41ec",
   "metadata": {},
   "source": [
    "# ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6419eaeb-c6e3-42e4-b529-5c04e0618538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.maximum(0, self.inputs)\n",
    "\n",
    "    def backward(self, dL_da):\n",
    "        self.dL_dz = dL_da.copy()\n",
    "        self.dL_dz[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a65e556-3c3b-4432-8e29-d3edab39474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(self.inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(self.inputs, axis=1, keepdims=True)\n",
    "        self.outputs = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3ae63f1-da62-4a60-84e9-2b818ccfc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 Regularization\n",
    "        if layer.weight_regularizer_l1>0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weigts))\n",
    "        if layer.bias_regularizer_l1>0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 Regularization\n",
    "        if layer.weight_regularizer_l2>0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        if layer.bias_regularizer_l2>0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "    \n",
    "    def calculate(self, y_pred, y_true):\n",
    "        neg_log_likelihoods = self.forward(y_pred, y_true)\n",
    "        avg_loss = np.mean(neg_log_likelihoods)\n",
    "        return avg_loss\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_predictions = y_pred_clipped[range(len(y_pred_clipped)), y_true]\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            correct_predictions = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        neg_log_likelihoods = -np.log(correct_predictions)\n",
    "        return neg_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291690e-9552-4dc3-823b-17a7af1a6a24",
   "metadata": {},
   "source": [
    "# Combined Softmax Activation and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa8bf1a5-d5ad-4eeb-91c8-009d055df170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actication_Softmax_Loss_Categorical_Cross_Entropy:\n",
    "    def __init__(self):\n",
    "        self.activation_softmax = Activation_Softmax()\n",
    "        self.loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation_softmax.forward(inputs)\n",
    "        self.softmax_outputs = self.activation_softmax.outputs\n",
    "        self.loss = self.loss_function(self.softmax_outputs, y_true)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        self.no_of_batches = len(y_pred)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dL_dz = self.softmax_outputs.copy()\n",
    "        self.dL_dz[range(no_of_batches), y_true] -= 1\n",
    "        self.dL_dz = self.dL_dz/no_of_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cfde8-0e9d-47f7-971a-838be22af4c0",
   "metadata": {},
   "source": [
    "# ADAM Optimizer with decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75a1e1eb-1040-453c-9aaa-e082483d8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optmizer_ADAM:\n",
    "    def __init__(self, learning_rate=.001, decay=0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate # alpha\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay # learning rate decay factor\n",
    "        self.epsilon = epsilon # To avoid division by zero\n",
    "        self.beta_1 = beta_1 # momentum factor\n",
    "        self.beta_2 = beta_2 # rho: cache memory decay rate\n",
    "        self.epoch = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        self.current_learning_rate = self.learning_rate / (1. + (self.decay * self.epoch))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentum = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentum = self.beta_1*layer.weight_momentum + (1-self.beta_1)*layer.dL_dw\n",
    "        layer.bias_momentum = self.beta_1*layer.bias_momentum + (1-self.beta_1)*layer.dL_db\n",
    "\n",
    "        layer.weight_cache = self.beta_2*layer.weight_cache + (1-self.beta_2)*(layer.dL_dw**2)\n",
    "        layer.bias_cache = self.beta_2*layer.bias_cache + (1-self.beta_2)*(layer.dL_db**2)\n",
    "\n",
    "        layer.weights += -self.current_learning_rate*(layer.weight_momentum/(1-self.beta_1**(self.epoch+1)))/\\\n",
    "                                                   (np.sqrt(layer.weight_cache/(1-self.beta_2**(self.epoch+1))) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate*(layer.bias_momentum/(1-self.beta_1**(self.epoch+1)))/\\\n",
    "                                                   (np.sqrt(layer.bias_cache/(1-self.beta_2**(self.epoch+1))) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
